{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Data\n",
    "\n",
<<<<<<< HEAD
    "In this notebook you will learn how to:\n",
    "\n",
    "1. Read data in a job\n",
    "1. Read *and* write data in a job\n",
    "1. Register data as an asset in Azure Machine Learning\n",
    "1. Read registered data assets from Azure Machine Learning in a job\n",
    "\n",
    "## Connect to Azure Machine Learning Workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace.\n",
=======
    "In this notebook you will learn how to use the AzureML SDK to:\n",
    "\n",
    "1. Read/write data in a job.\n",
    "1. Create a data asset to share with others in your team.\n",
    "1. Abstract schema for tabular data using `MLTable`.\n",
    "\n",
    "## Connect to Azure Machine Learning Workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace.\n",
>>>>>>> main
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "from azure.ml import MLClient\n",
=======
    "from azure.ai.ml import MLClient\n",
>>>>>>> main
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# enter details of your AML workspace\n",
    "subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "resource_group = \"<RESOURCE_GROUP>\"\n",
    "workspace = \"<AML_WORKSPACE_NAME>\"\n",
    "\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Reading data in a job\n",
=======
    "## Reading/writing data in a job\n",
>>>>>>> main
    "\n",
    "In this example we will use the titanic dataset in this repo - ([./sample_data/titanic.csv](./sample_data/titanic.csv)) and set-up a command that executes the following python code:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(args.input_data)\n",
    "print(df.head(10))\n",
    "```\n",
    "\n",
    "Below is the code for submitting the command to the cloud - note that both the code *and* the data is automatically uploaded to the cloud. Note: The data is only re-uploaded on subsequent job submissions if data has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "from azure.ml import command\n",
    "from azure.ml.entities import Data, UriReference\n",
    "from azure.ml import Input\n",
    "from azure.ml.constants import AssetTypes\n",
    "\n",
    "my_job_inputs = {\n",
=======
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# === Note on path ===\n",
    "# can be can be a local path or a cloud path. AzureML supports https://`, `abfss://`, `wasbs://` and `azureml://` URIs.\n",
    "# Local paths are automatically uploaded to the default datastore in the cloud.\n",
    "# More details on supported paths: https://docs.microsoft.com/azure/machine-learning/how-to-read-write-data-v2#supported-paths\n",
    "\n",
    "inputs = {\n",
>>>>>>> main
    "    \"input_data\": Input(type=AssetTypes.URI_FILE, path=\"./sample_data/titanic.csv\")\n",
    "}\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",  # local path where the code is stored\n",
    "    command=\"python read_data.py --input_data ${{inputs.input_data}}\",\n",
<<<<<<< HEAD
    "    inputs=my_job_inputs,\n",
=======
    "    inputs=inputs,\n",
>>>>>>> main
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:9\",\n",
    "    compute=\"cpu-cluster\",\n",
    ")\n",
    "\n",
    "# submit the command\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "# get a URL for the status of the job\n",
    "returned_job.services[\"Studio\"].endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Understanding the code\n",
    "\n",
    "When the job has executed, you will see in the log files a print out of the first 10 records of the titanic sample data. The cell above you can see the inputs to the job were defined using a `dict`:\n",
    "\n",
    "```python\n",
    "my_job_inputs = {\n",
    "    \"input_data\": Input(\n",
    "        type=AssetTypes.URI_FILE, \n",
    "        path='./sample_data/titanic.csv'\n",
    "    )\n",
    "}\n",
    "```\n",
    "\n",
    "The `Input` class allow you to define data inputs where:\n",
    "\n",
    "- `type` can be a `uri_file` (a specific file) or `uri_folder` (a folder location)\n",
    "- `path` can be a local path or a cloud path. Azure Machine Learning supports `https://`, `abfss://`, `wasbs://` and `azureml://` URIs. As you saw above, if the path is local but your compute is defined to be in the cloud, Azure Machine Learning will automatically upload the data to cloud storage for you.\n",
    "\n",
    "The `Input` defaults the `mode` - how the input will be exposed during job runtime - to `InputOutputModes.RO_MOUNT` (read-only mount). Put another way, Azure Machine Learning will mount the file or folder to the compute and set the file/folder to read-only. By design, you cannot *write* to `JobInputs` only `JobOutputs` - we will cover this later in the notebook.\n",
    "\n",
    "#### Accessing data already in the cloud\n",
    "\n",
    "As mentioned above, the `path` in Input supports `https://`, `abfss://`, `wasbs://` and `azureml://` protocols. Therefore, you can simply change the `path` in the above cell to a cloud-based URI."
=======
    "### Reading *and* writing data in a job\n",
    "\n",
    "By design, you cannot *write* to `Inputs` only `Outputs`. The code below creates an `Output` that will mount your AzureML default datastore (Azure Blob) in Read-*Write* mode. The python code simply takes the CSV as import and exports it as a parquet file, i.e.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(args.input_data)\n",
    "output_path = os.path.join(args.output_folder, \"my_output.parquet\")\n",
    "df.to_parquet(output_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "inputs = {\n",
    "    \"input_data\": Input(type=AssetTypes.URI_FILE, path=\"./sample_data/titanic.csv\")\n",
    "}\n",
    "\n",
    "outputs = {\n",
    "    \"output_folder\": Output(\n",
    "        type=AssetTypes.URI_FOLDER,\n",
    "        path=f\"azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/workspaceblobstore/paths/\",\n",
    "    )\n",
    "}\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",  # local path where the code is stored\n",
    "    command=\"python read_write_data.py --input_data ${{inputs.input_data}} --output_folder ${{outputs.output_folder}}\",\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:9\",\n",
    "    compute=\"cpu-cluster\",\n",
    ")\n",
    "\n",
    "# submit the command\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "# get a URL for the status of the job\n",
    "returned_job.services[\"Studio\"].endpoint"
>>>>>>> main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Reading *and* writing data in a job\n",
    "\n",
    "By design, you cannot *write* to `JobInputs` only `JobOutputs`. Say, you want to read in some data, do some processing and then write the processed data back to the cloud. In the example below you get the URI of the default Azure ML datastore:"
=======
    "## Create Data Assets\n",
    "\n",
    "You can create a data asset in Azure Machine Learning, which has the following benefits:\n",
    "\n",
    "- Easy to share with other members of the team (no need to remember file locations)\n",
    "- Versioning of the metadata (location, description, etc)\n",
    "- Lineage tracking\n",
    "\n",
    "Below we show an example of versioning the sample data in this repo. The data is uploaded to cloud storage and registered as an asset."
>>>>>>> main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "default_dstor = ml_client.datastores.get_default()\n",
    "output_path = (\n",
    "    default_dstor.protocol\n",
    "    + \"://\"\n",
    "    + default_dstor.account_name\n",
    "    + \".blob.\"\n",
    "    + default_dstor.endpoint\n",
    "    + \"/\"\n",
    "    + default_dstor.container_name\n",
    ")\n",
    "\n",
    "print(output_path)"
=======
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "try:\n",
    "    registered_data_asset = ml_client.data.get(name=\"titanic\", version=\"1\")\n",
    "    print(\"Found data asset. Will not create again\")\n",
    "except Exception as ex:\n",
    "    my_data = Data(\n",
    "        path=\"./sample_data/titanic.csv\",\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        description=\"Titanic Data\",\n",
    "        name=\"titanic\",\n",
    "        version=\"1\",\n",
    "    )\n",
    "    ml_client.data.create_or_update(my_data)\n",
    "    registered_data_asset = ml_client.data.get(name=\"titanic\", version=\"1\")\n",
    "    print(\"Created data asset\")"
>>>>>>> main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "The code below creates a `Output` that will mount your Azure Machine Learning default storage (Azure Blob) in Read-*Write* mode. The python code simply takes the CSV as import and exports it as a parquet file, i.e.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(args.input_data)\n",
    "output_path = os.path.join(args.output_folder, \"my_output.parquet\")\n",
    "df.to_parquet(output_path)\n",
    "```"
=======
    "> Note: Whilst the above example shows a local file. Remember that `path` supports cloud storage (`https`, `abfss`, `wasbs` protocols). Therefore, if you want to register data in a cloud location just specify the path with any of the supported protocols.\n",
    "\n",
    "### Consume data assets in a job\n",
    "\n",
    "Below shows how to consume a data asset in the job:\n"
>>>>>>> main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "from azure.ml import command\n",
    "from azure.ml.entities import Data, UriReference\n",
    "from azure.ml import Input, Output\n",
    "from azure.ml.constants import AssetTypes\n",
    "\n",
    "my_job_inputs = {\n",
    "    \"input_data\": Input(type=AssetTypes.URI_FILE, path=\"./sample_data/titanic.csv\")\n",
    "}\n",
    "\n",
    "my_job_outputs = {\"output_folder\": Output(type=AssetTypes.URI_FOLDER, path=output_path)}\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",  # local path where the code is stored\n",
    "    command=\"python read_write_data.py --input_data ${{inputs.input_data}} --output_folder ${{outputs.output_folder}}\",\n",
    "    inputs=my_job_inputs,\n",
    "    outputs=my_job_outputs,\n",
=======
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "registered_data_asset = ml_client.data.get(name=\"titanic\", version=\"1\")\n",
    "\n",
    "my_job_inputs = {\n",
    "    \"input_data\": Input(type=AssetTypes.URI_FILE, path=registered_data_asset.id)\n",
    "}\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",\n",
    "    command=\"python read_data.py --input_data ${{inputs.input_data}}\",\n",
    "    inputs=my_job_inputs,\n",
>>>>>>> main
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:9\",\n",
    "    compute=\"cpu-cluster\",\n",
    ")\n",
    "\n",
    "# submit the command\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "# get a URL for the status of the job\n",
    "returned_job.services[\"Studio\"].endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Registering data as an asset in Azure Machine Learning\n",
    "\n",
    "You can register data as an asset in Azure Machine Learning. The benefits of registering data are:\n",
    "\n",
    "- Easy to share with other members of the team (no need to remember file locations)\n",
    "- Versioning of the metadata (location, description, etc)\n",
    "- Lineage tracking\n",
    "\n",
    "Below we show an example of versioning the sample data in this repo. The data is uploaded to cloud storage and registered as an asset."
=======
    "## MLTable\n",
    "\n",
    "`MLTable` is a way to abstract the schema definition for tabular data so that it is easier for consumers of the data to materialize the table into a Pandas/Dask/Spark dataframe. [A more detailed explanation and motivation is provided on docs.microsoft.com.](https://docs.microsoft.com/azure/machine-learning/concept-data#mltable).\n",
    "\n",
    "The ideal scenarios to use `MLTable` are:\n",
    "\n",
    "- The schema of your data is complex and/or changes frequently.\n",
    "- You only need a subset of data (for example: a sample of rows or files, specific columns, etc).\n",
    "- AutoML jobs requiring tabular data.\n",
    "\n",
    "If your scenario does not fit the above then it is likely that URIs are a more suitable type.\n",
    "\n",
    "### The `MLTable` file\n",
    "\n",
    "The `MLTable` file defines the schema for tabular data. Below is a sample:"
>>>>>>> main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "from azure.ml.entities import Data\n",
    "from azure.ml.constants import AssetTypes\n",
    "\n",
    "my_data = Data(\n",
    "    path=\"./sample_data/titanic.csv\",\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"Titanic Data\",\n",
    "    name=\"titanic\",\n",
    "    version=\"1\",\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
=======
    "! cat ./sample-mltable/MLTable"
>>>>>>> main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "> Note: Whilst the above example shows a local file. Remember that `path` supports cloud storage (`https`, `abfss`, `wasbs` protocols). Therefore, if you want to register data in a cloud location just specify the path with any of the supported protocols.\n",
    "\n",
    "### Consume data assets in an Azure Machine Learning Job\n",
    "\n",
    "Below we use the previously registered data asset in the job by refering to the long-form ID in the `path`:\n",
    "\n",
    "```txt\n",
    "/subscriptions/XXXXX/resourceGroups/XXXXX/providers/Microsoft.MachineLearningServices/workspaces/XXXXX/datasets/titanic/versions/1\n",
    "```\n",
    "\n",
    "This long-form URI is accessed using:\n",
    "\n",
    "```python\n",
    "registered_data_asset = ml_client.data.get(name='titanic', version='1')\n",
    "registered_data_asset.id\n",
    "```\n"
=======
    "We recommend that you co-locate your `MLTable` file with the underlying data (i.e. the `MLTable` file should be in the same (or parent) directory. You can can load an `MLTable` artefact using the `mltable` library - below below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mltable\n",
    "\n",
    "# Note: the uri below can be a local folder or folder located in cloud storage. The folder must contain a valid MLTable file.\n",
    "tbl = mltable.load(uri=\"./sample-mltable\")\n",
    "tbl.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read an MLTable in a job\n",
    "\n",
    "#### Create an environment\n",
    "\n",
    "Firstly, you need to create an environment that contains the mltable Python Library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "env_docker_conda = Environment(\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
    "    conda_file=\"env-mltable.yml\",\n",
    "    name=\"mltable\",\n",
    "    description=\"Environment created for consuming MLTable.\",\n",
    ")\n",
    "\n",
    "ml_client.environments.create_or_update(env_docker_conda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a job"
>>>>>>> main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "from azure.ml import command, Input, Output\n",
    "from azure.ml.entities import Data, UriReference\n",
    "from azure.ml.constants import AssetTypes\n",
    "\n",
    "registered_data_asset = ml_client.data.get(name=\"titanic\", version=\"1\")\n",
    "\n",
    "my_job_inputs = {\n",
    "    \"input_data\": Input(type=AssetTypes.URI_FILE, path=registered_data_asset.id)\n",
    "}\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",\n",
    "    command=\"python read_data_asset.py --input_data ${{inputs.input_data}}\",\n",
    "    inputs=my_job_inputs,\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:9\",\n",
=======
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "inputs = {\"input_data\": Input(type=AssetTypes.MLTABLE, path=\"./sample-mltable\")}\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",  # local path where the code is stored\n",
    "    command=\"python read_mltable.py --input_data ${{inputs.input_data}}\",\n",
    "    inputs=inputs,\n",
    "    environment=env_docker_conda,\n",
>>>>>>> main
    "    compute=\"cpu-cluster\",\n",
    ")\n",
    "\n",
    "# submit the command\n",
<<<<<<< HEAD
    "returned_job = ml_client.create_or_update(job)\n",
=======
    "returned_job = ml_client.jobs.create_or_update(job)\n",
>>>>>>> main
    "# get a URL for the status of the job\n",
    "returned_job.services[\"Studio\"].endpoint"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Read, write and register a data asset"
  },
<<<<<<< HEAD
  "interpreter": {
   "hash": "66962d4c952b5ba37638a017d6cc83bab37d76f69b13c17d86b9f71233a0aa71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
=======
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
>>>>>>> main
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.12"
=======
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0ece9b0d22cc202945275ade981e664a4c51236f9f70f1a68cccb779b759da7e"
   }
>>>>>>> main
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
<<<<<<< HEAD
}
=======
}
>>>>>>> main
